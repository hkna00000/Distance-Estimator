{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hkna0\\anaconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     filename       class  truncated  occluded  observation angle    xmin  \\\n",
      "0  000000.txt  Pedestrian        0.0         0              -0.20  712.40   \n",
      "1  000001.txt       Truck        0.0         0              -1.57  599.41   \n",
      "2  000001.txt         Car        0.0         0               1.85  387.63   \n",
      "3  000001.txt     Cyclist        0.0         3              -1.65  676.60   \n",
      "4  000002.txt        Misc        0.0         0              -1.82  804.79   \n",
      "\n",
      "     ymin    xmax    ymax  height  width  length   xloc  yloc   zloc  rot_y  \n",
      "0  143.00  810.73  307.92    1.89   0.48    1.20   1.84  1.47   8.41   0.01  \n",
      "1  156.40  629.75  189.25    2.85   2.63   12.34   0.47  1.49  69.44  -1.56  \n",
      "2  181.54  423.81  203.12    1.67   1.87    3.69 -16.53  2.39  58.49   1.57  \n",
      "3  163.95  688.98  193.93    1.86   0.60    2.02   4.59  1.32  45.84  -1.55  \n",
      "4  167.34  995.43  327.94    1.63   1.48    2.37   3.23  1.59   8.55  -1.47  \n"
     ]
    }
   ],
   "source": [
    "# Load the annotation file\n",
    "annotations = pd.read_csv(\"../classification_annotation/anno_classi.csv\")\n",
    "\n",
    "# Example structure\n",
    "print(annotations.head())  # Ensure the file is read correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hkna0\\anaconda3\\envs\\py310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hkna0\\anaconda3\\envs\\py310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 loaded for feature extraction.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained ResNet model\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet = nn.Sequential(*list(resnet.children())[:-1])  # Remove the last classification layer\n",
    "resnet.eval()  # Set model to evaluation mode\n",
    "\n",
    "print(\"ResNet50 loaded for feature extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform for image input\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ResNet expects 224x224 images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Paths\n",
    "image_folder = \"../original_data/train_images\"\n",
    "annotations_file = \"../classification_annotation/anno_classi.csv\"  # Update with your annotation file path\n",
    "features_output_file = \"../classification_annotation/features.csv\"\n",
    "labels_output_file = \"../classification_annotation/labels.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKIP THIS STEP IF YOU ALREADY GOT THE FEATURE EXTRACTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load annotations\n",
    "# annotations = pd.read_csv(annotations_file)\n",
    "\n",
    "# # Replace .txt with .png in the filename column\n",
    "# annotations[\"filename\"] = annotations[\"filename\"].str.replace(\".txt\", \".png\", regex=False)\n",
    "\n",
    "# # Initialize storage for features and labels\n",
    "# features = []\n",
    "# labels = []\n",
    "\n",
    "# # Process each annotation\n",
    "# for idx, row in annotations.iterrows():\n",
    "#     image_path = os.path.join(image_folder, row[\"filename\"])\n",
    "    \n",
    "#     if os.path.exists(image_path):\n",
    "#         try:\n",
    "#             # Load and process the image\n",
    "#             image = Image.open(image_path).convert(\"RGB\")\n",
    "#             xmin, ymin, xmax, ymax = map(int, [row[\"xmin\"], row[\"ymin\"], row[\"xmax\"], row[\"ymax\"]])\n",
    "#             # Crop to bounding box\n",
    "#             cropped_image = image.crop((xmin, ymin, xmax, ymax))\n",
    "#             cropped_image = transform(cropped_image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "#             # Extract features using ResNet\n",
    "#             with torch.no_grad():\n",
    "#                 feature = resnet(cropped_image).squeeze().numpy()\n",
    "            \n",
    "#             # Append features and label\n",
    "#             features.append(feature)\n",
    "#             labels.append(row[\"class\"])  # Store the class label\n",
    "#             print(f\"Processed image {row['filename']} with class {row['class']}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing image {row['filename']}: {e}\")\n",
    "#     else:\n",
    "#         print(f\"Image not found: {image_path}\")\n",
    "\n",
    "# # Save features and labels to CSV files\n",
    "# features = np.array(features)\n",
    "# labels = np.array(labels)\n",
    "\n",
    "# # Save features and labels to CSV files\n",
    "# np.savetxt(features_output_file, features, delimiter=\",\")\n",
    "# np.savetxt(labels_output_file, labels, fmt=\"%s\")\n",
    "\n",
    "# print(f\"Feature extraction complete. Saved {len(features)} features and {len(labels)} labels.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "           Car       0.88      0.99      0.93      5661\n",
      "       Cyclist       0.96      0.47      0.63       333\n",
      "          Misc       1.00      0.31      0.47       208\n",
      "    Pedestrian       0.85      0.96      0.91       934\n",
      "Person_sitting       1.00      0.50      0.67        40\n",
      "          Tram       0.99      0.80      0.89       102\n",
      "         Truck       0.98      0.59      0.74       230\n",
      "           Van       0.93      0.31      0.47       606\n",
      "\n",
      "      accuracy                           0.88      8114\n",
      "     macro avg       0.95      0.62      0.71      8114\n",
      "  weighted avg       0.89      0.88      0.86      8114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load features and labels\n",
    "features = np.loadtxt(\"../classification_annotation/features.csv\", delimiter=\",\")\n",
    "labels = np.loadtxt(\"../classification_annotation/labels.csv\", dtype=str)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load extracted features and labels\n",
    "features = np.loadtxt(features_output_file, delimiter=\",\")\n",
    "labels = np.loadtxt(labels_output_file, dtype=str)\n",
    "\n",
    "# Encode labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the LabelEncoder\n",
    "with open(\"../scaler/label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# Save class names to a .npy file\n",
    "np.save(\"../scaler/class_names.npy\", label_encoder.classes_)\n",
    "# Convert to torch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.4747, Train Accuracy: 0.8503\n",
      "Val Loss: 0.3617, Val Accuracy: 0.8749\n",
      "Epoch 2/10\n",
      "Train Loss: 0.3503, Train Accuracy: 0.8837\n",
      "Val Loss: 0.3047, Val Accuracy: 0.8950\n",
      "Epoch 3/10\n",
      "Train Loss: 0.3163, Train Accuracy: 0.8955\n",
      "Val Loss: 0.2674, Val Accuracy: 0.9103\n",
      "Epoch 4/10\n",
      "Train Loss: 0.2956, Train Accuracy: 0.8987\n",
      "Val Loss: 0.2700, Val Accuracy: 0.9095\n",
      "Epoch 5/10\n",
      "Train Loss: 0.2778, Train Accuracy: 0.9055\n",
      "Val Loss: 0.2503, Val Accuracy: 0.9125\n",
      "Epoch 6/10\n",
      "Train Loss: 0.2657, Train Accuracy: 0.9100\n",
      "Val Loss: 0.2396, Val Accuracy: 0.9184\n",
      "Epoch 7/10\n",
      "Train Loss: 0.2522, Train Accuracy: 0.9139\n",
      "Val Loss: 0.2379, Val Accuracy: 0.9152\n",
      "Epoch 8/10\n",
      "Train Loss: 0.2486, Train Accuracy: 0.9133\n",
      "Val Loss: 0.2283, Val Accuracy: 0.9205\n",
      "Epoch 9/10\n",
      "Train Loss: 0.2354, Train Accuracy: 0.9191\n",
      "Val Loss: 0.2031, Val Accuracy: 0.9286\n",
      "Epoch 10/10\n",
      "Train Loss: 0.2248, Train Accuracy: 0.9223\n",
      "Val Loss: 0.2211, Val Accuracy: 0.9231\n"
     ]
    }
   ],
   "source": [
    "# Define a simple neural network model for classification\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]  # Number of features (from ResNet output)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = SimpleNN(input_dim, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_preds += (preds == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct_preds = 0\n",
    "    val_total_preds = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct_preds += (preds == labels).sum().item()\n",
    "            val_total_preds += labels.size(0)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {correct_preds/total_preds:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {val_correct_preds/val_total_preds:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2211\n",
      "Validation Accuracy: 0.9231\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "val_loss = 0.0\n",
    "correct_preds = 0\n",
    "total_preds = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_preds += (preds == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "print(f\"Validation Loss: {val_loss / len(val_loader):.4f}\")\n",
    "print(f\"Validation Accuracy: {correct_preds / total_preds:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model state_dict\n",
    "torch.save(model.state_dict(), \"../model/simple_nn_classi.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2048])\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels, transform=None):\n",
    "        # Ensure that the features and labels are numpy arrays (2D for features, 1D for labels)\n",
    "        self.features = np.array(features)\n",
    "        self.labels = np.array(labels).flatten()  # Make sure the labels are 1D\n",
    "\n",
    "        # Encode labels \n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
    "\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            feature = self.transform(feature)\n",
    "        return feature, label\n",
    "\n",
    "features = np.loadtxt(\"../classification_annotation/features.csv\", delimiter=\",\")\n",
    "labels = np.loadtxt(\"../classification_annotation/labels.csv\", dtype=str)\n",
    "\n",
    "dataset = CustomDataset(features, labels)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "# Check if the dataset is loading correctly\n",
    "for inputs, labels in train_loader:\n",
    "    print(inputs.shape)  # Should print something like [batch_size, feature_size]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomResNetModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(CustomResNetModel, self).__init__()\n",
    "        # Fully connected layer to match the number of input features\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_size = 2048  \n",
    "num_classes = len(set(dataset.labels))  # Number of unique classes\n",
    "\n",
    "# Initialize the model\n",
    "model = CustomResNetModel(input_size, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.0668, Train Accuracy: 0.9761\n",
      "Epoch 2/10\n",
      "Train Loss: 0.0579, Train Accuracy: 0.9797\n",
      "Epoch 3/10\n",
      "Train Loss: 0.0538, Train Accuracy: 0.9812\n",
      "Epoch 4/10\n",
      "Train Loss: 0.0415, Train Accuracy: 0.9861\n",
      "Epoch 5/10\n",
      "Train Loss: 0.0377, Train Accuracy: 0.9864\n",
      "Epoch 6/10\n",
      "Train Loss: 0.0354, Train Accuracy: 0.9876\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0355, Train Accuracy: 0.9884\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0273, Train Accuracy: 0.9909\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0336, Train Accuracy: 0.9871\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0301, Train Accuracy: 0.9893\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_preds += (preds == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "    # Calculate the accuracy and loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct_preds / total_preds\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"../model/resnetmodel.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
